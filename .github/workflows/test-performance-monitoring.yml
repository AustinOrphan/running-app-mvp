name: 📊 Test Performance Monitoring

on:
  # Run after main CI workflows complete
  workflow_run:
    workflows: ['🚀 CI Pipeline']
    types: [completed]
    branches: [main]
    
  # Manual trigger for performance analysis
  workflow_dispatch:
    inputs:
      analysis_type:
        description: 'Type of analysis to perform'
        required: true
        default: 'full'
        type: choice
        options:
          - 'full'
          - 'collect-only'
          - 'trends-only'
          - 'alerts-only'
      generate_dashboard:
        description: 'Generate performance dashboard'
        required: false
        default: true
        type: boolean

# Allow only one performance monitoring run at a time
concurrency:
  group: performance-monitoring
  cancel-in-progress: false

env:
  NODE_VERSION: '20'
  PERFORMANCE_THRESHOLD_UNIT: 30000      # 30 seconds
  PERFORMANCE_THRESHOLD_INTEGRATION: 120000  # 2 minutes
  PERFORMANCE_THRESHOLD_E2E: 300000     # 5 minutes

jobs:
  # Collect performance metrics
  collect-metrics:
    name: 📊 Collect Performance Metrics
    runs-on: ubuntu-latest
    if: github.event.inputs.analysis_type != 'trends-only' && github.event.inputs.analysis_type != 'alerts-only'
    outputs:
      metrics-collected: ${{ steps.collect.outputs.collected }}
      performance-summary: ${{ steps.collect.outputs.summary }}
      
    steps:
      - name: 📥 Checkout code
        uses: actions/checkout@v4
        
      - name: 📦 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          
      - name: 🔧 Install dependencies
        run: npm ci
        
      # Create reports directory
      - name: 📁 Create reports directory
        run: mkdir -p reports/performance
        
      # Download previous metrics if available
      - name: 📥 Download previous metrics
        uses: actions/download-artifact@v4
        with:
          name: performance-metrics
          path: reports/performance
        continue-on-error: true
        
      # Run performance collection
      - name: 📊 Collect test performance metrics
        id: collect
        run: |
          echo "🚀 Starting performance metrics collection..."
          
          # Run the performance dashboard script to collect metrics
          node scripts/test-performance-dashboard.js --collect --output ./reports/performance
          
          # Check if metrics were collected successfully
          if [ -f "reports/performance/metrics.json" ]; then
            echo "collected=true" >> $GITHUB_OUTPUT
            
            # Extract summary information
            LATEST_METRICS=$(tail -1 reports/performance/metrics.json | jq -r '.overall')
            TOTAL_DURATION=$(echo "$LATEST_METRICS" | jq -r '.totalDuration')
            SUCCESS_RATE=$(echo "$LATEST_METRICS" | jq -r '(.totalPassed / .totalTests) * 100')
            
            echo "summary=Duration: ${TOTAL_DURATION}ms, Success: ${SUCCESS_RATE}%" >> $GITHUB_OUTPUT
            
            echo "✅ Performance metrics collected successfully"
          else
            echo "collected=false" >> $GITHUB_OUTPUT
            echo "❌ Failed to collect performance metrics"
          fi
          
      # Upload metrics as artifact
      - name: 📤 Upload performance metrics
        if: steps.collect.outputs.collected == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: performance-metrics
          path: reports/performance/metrics.json
          retention-days: 90

  # Analyze performance trends
  analyze-trends:
    name: 📈 Analyze Performance Trends
    runs-on: ubuntu-latest
    needs: [collect-metrics]
    if: always() && (needs.collect-metrics.outputs.metrics-collected == 'true' || github.event.inputs.analysis_type == 'trends-only')
    outputs:
      trends-analyzed: ${{ steps.trends.outputs.analyzed }}
      degradation-detected: ${{ steps.trends.outputs.degradation }}
      
    steps:
      - name: 📥 Checkout code
        uses: actions/checkout@v4
        
      - name: 📦 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          
      - name: 🔧 Install dependencies
        run: npm ci
        
      - name: 📁 Create reports directory
        run: mkdir -p reports/performance
        
      # Download metrics for analysis
      - name: 📥 Download performance metrics
        uses: actions/download-artifact@v4
        with:
          name: performance-metrics
          path: reports/performance
        continue-on-error: true
        
      # Analyze trends
      - name: 📈 Analyze performance trends
        id: trends
        run: |
          echo "📈 Analyzing performance trends..."
          
          # Run trend analysis
          node scripts/test-performance-dashboard.js --analyze --output ./reports/performance
          
          if [ -f "reports/performance/trends.json" ]; then
            echo "analyzed=true" >> $GITHUB_OUTPUT
            
            # Check for significant degradation
            DEGRADATION=$(jq -r '[.analysis[] | select(.duration.direction == "degrading" and (.duration.changePercent > 20))] | length > 0' reports/performance/trends.json)
            echo "degradation=$DEGRADATION" >> $GITHUB_OUTPUT
            
            echo "✅ Trend analysis completed"
          else
            echo "analyzed=false" >> $GITHUB_OUTPUT
            echo "❌ Trend analysis failed"
          fi
          
      # Upload trends
      - name: 📤 Upload trend analysis
        if: steps.trends.outputs.analyzed == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: performance-trends
          path: reports/performance/trends.json
          retention-days: 90

  # Check for performance alerts
  check-alerts:
    name: 🚨 Check Performance Alerts
    runs-on: ubuntu-latest
    needs: [collect-metrics, analyze-trends]
    if: always() && (needs.collect-metrics.outputs.metrics-collected == 'true' || github.event.inputs.analysis_type == 'alerts-only')
    outputs:
      alerts-found: ${{ steps.alerts.outputs.found }}
      critical-alerts: ${{ steps.alerts.outputs.critical }}
      
    steps:
      - name: 📥 Checkout code
        uses: actions/checkout@v4
        
      - name: 📦 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          
      - name: 🔧 Install dependencies
        run: npm ci
        
      - name: 📁 Create reports directory
        run: mkdir -p reports/performance
        
      # Download metrics and trends
      - name: 📥 Download performance data
        uses: actions/download-artifact@v4
        with:
          name: performance-metrics
          path: reports/performance
        continue-on-error: true
        
      - name: 📥 Download trend data
        uses: actions/download-artifact@v4
        with:
          name: performance-trends
          path: reports/performance
        continue-on-error: true
        
      # Check for alerts
      - name: 🚨 Check performance alerts
        id: alerts
        run: |
          echo "🚨 Checking for performance alerts..."
          
          # Run alert checking
          node scripts/test-performance-dashboard.js --alert --output ./reports/performance
          
          if [ -f "reports/performance/alerts.json" ]; then
            ALERT_COUNT=$(jq 'length' reports/performance/alerts.json)
            CRITICAL_COUNT=$(jq '[.[] | select(.severity == "high")] | length' reports/performance/alerts.json)
            
            echo "found=$ALERT_COUNT" >> $GITHUB_OUTPUT
            echo "critical=$CRITICAL_COUNT" >> $GITHUB_OUTPUT
            
            if [ "$ALERT_COUNT" -gt 0 ]; then
              echo "⚠️ Found $ALERT_COUNT performance alerts ($CRITICAL_COUNT critical)"
            else
              echo "✅ No performance alerts detected"
            fi
          else
            echo "found=0" >> $GITHUB_OUTPUT
            echo "critical=0" >> $GITHUB_OUTPUT
          fi
          
      # Create GitHub issue for critical alerts
      - name: 🎫 Create alert issue
        if: steps.alerts.outputs.critical > 0
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            if (fs.existsSync('reports/performance/alerts.json')) {
              const alerts = JSON.parse(fs.readFileSync('reports/performance/alerts.json', 'utf8'));
              const criticalAlerts = alerts.filter(alert => alert.severity === 'high');
              
              if (criticalAlerts.length > 0) {
                const body = `## 🚨 Critical Performance Alerts Detected

                **Alert Count**: ${criticalAlerts.length}
                **Detection Time**: ${new Date().toISOString()}
                **Workflow**: [${context.workflow}](${context.payload.repository.html_url}/actions/runs/${context.runId})

### Critical Alerts:

${criticalAlerts.map(alert => `
- **${alert.message}**
  - Suite: ${alert.suite}
  - Type: ${alert.type}
  ${alert.actual ? `- Actual: ${alert.actual}` : ''}
  ${alert.threshold ? `- Threshold: ${alert.threshold}` : ''}
  ${alert.changePercent ? `- Change: ${alert.changePercent > 0 ? '+' : ''}${alert.changePercent.toFixed(1)}%` : ''}
`).join('')}

### Recommended Actions:
1. Review test performance metrics in the dashboard
2. Investigate slow or failing tests
3. Check for recent changes that might impact performance
4. Consider optimizing test setup or infrastructure

### Dashboard Links:
- [Performance Dashboard](${context.payload.repository.html_url}/actions/artifacts)
- [Workflow Run](${context.payload.repository.html_url}/actions/runs/${context.runId})
`;

                await github.rest.issues.create({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  title: `🚨 Critical Test Performance Alerts - ${new Date().toLocaleDateString()}`,
                  body: body,
                  labels: ['performance', 'alert', 'critical', 'testing']
                });
              }
            }
            
      # Upload alerts
      - name: 📤 Upload alerts
        uses: actions/upload-artifact@v4
        with:
          name: performance-alerts
          path: reports/performance/alerts.json
          retention-days: 90

  # Generate performance dashboard
  generate-dashboard:
    name: 🎨 Generate Performance Dashboard
    runs-on: ubuntu-latest
    needs: [collect-metrics, analyze-trends, check-alerts]
    if: always() && (github.event.inputs.generate_dashboard == 'true' || github.event.inputs.generate_dashboard == '')
    
    steps:
      - name: 📥 Checkout code
        uses: actions/checkout@v4
        
      - name: 📦 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          
      - name: 🔧 Install dependencies
        run: npm ci
        
      - name: 📁 Create reports directory
        run: mkdir -p reports/performance
        
      # Download all performance data
      - name: 📥 Download performance metrics
        uses: actions/download-artifact@v4
        with:
          name: performance-metrics
          path: reports/performance
        continue-on-error: true
        
      - name: 📥 Download trend analysis
        uses: actions/download-artifact@v4
        with:
          name: performance-trends
          path: reports/performance
        continue-on-error: true
        
      - name: 📥 Download alerts
        uses: actions/download-artifact@v4
        with:
          name: performance-alerts
          path: reports/performance
        continue-on-error: true
        
      # Generate dashboard
      - name: 🎨 Generate performance dashboard
        run: |
          echo "🎨 Generating performance dashboard..."
          
          # Generate the HTML dashboard
          node scripts/test-performance-dashboard.js --generate --output ./reports/performance
          
          if [ -f "reports/performance/dashboard.html" ]; then
            echo "✅ Dashboard generated successfully"
          else
            echo "❌ Dashboard generation failed"
            exit 1
          fi
          
      # Upload dashboard
      - name: 📤 Upload performance dashboard
        uses: actions/upload-artifact@v4
        with:
          name: performance-dashboard
          path: reports/performance/dashboard.html
          retention-days: 30
          
      # Deploy dashboard to GitHub Pages (optional)
      - name: 🚀 Deploy to GitHub Pages
        if: github.ref == 'refs/heads/main'
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./reports/performance
          destination_dir: performance
          keep_files: false

  # Performance monitoring summary
  monitoring-summary:
    name: 📋 Performance Monitoring Summary
    runs-on: ubuntu-latest
    needs: [collect-metrics, analyze-trends, check-alerts, generate-dashboard]
    if: always()
    
    steps:
      - name: 📊 Generate monitoring summary
        run: |
          echo "# 📊 Test Performance Monitoring Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Workflow**: Test Performance Monitoring" >> $GITHUB_STEP_SUMMARY
          echo "**Triggered by**: ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
          echo "**Branch**: ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
          echo "**Date**: $(date -u)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Metrics collection status
          if [ "${{ needs.collect-metrics.result }}" = "success" ]; then
            echo "✅ **Metrics Collection**: Completed" >> $GITHUB_STEP_SUMMARY
            echo "   - ${{ needs.collect-metrics.outputs.performance-summary }}" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ needs.collect-metrics.result }}" = "failure" ]; then
            echo "❌ **Metrics Collection**: Failed" >> $GITHUB_STEP_SUMMARY
          else
            echo "⏭️ **Metrics Collection**: Skipped" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Trend analysis status
          if [ "${{ needs.analyze-trends.result }}" = "success" ]; then
            echo "✅ **Trend Analysis**: Completed" >> $GITHUB_STEP_SUMMARY
            if [ "${{ needs.analyze-trends.outputs.degradation-detected }}" = "true" ]; then
              echo "   ⚠️ Performance degradation detected" >> $GITHUB_STEP_SUMMARY
            else
              echo "   📈 Performance trends within normal range" >> $GITHUB_STEP_SUMMARY
            fi
          elif [ "${{ needs.analyze-trends.result }}" = "failure" ]; then
            echo "❌ **Trend Analysis**: Failed" >> $GITHUB_STEP_SUMMARY
          else
            echo "⏭️ **Trend Analysis**: Skipped" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Alert checking status
          if [ "${{ needs.check-alerts.result }}" = "success" ]; then
            echo "✅ **Alert Checking**: Completed" >> $GITHUB_STEP_SUMMARY
            ALERT_COUNT="${{ needs.check-alerts.outputs.alerts-found }}"
            CRITICAL_COUNT="${{ needs.check-alerts.outputs.critical-alerts }}"
            
            if [ "$CRITICAL_COUNT" -gt 0 ]; then
              echo "   🚨 $CRITICAL_COUNT critical alerts found" >> $GITHUB_STEP_SUMMARY
            elif [ "$ALERT_COUNT" -gt 0 ]; then
              echo "   ⚠️ $ALERT_COUNT performance alerts found" >> $GITHUB_STEP_SUMMARY
            else
              echo "   ✅ No performance alerts" >> $GITHUB_STEP_SUMMARY
            fi
          elif [ "${{ needs.check-alerts.result }}" = "failure" ]; then
            echo "❌ **Alert Checking**: Failed" >> $GITHUB_STEP_SUMMARY
          else
            echo "⏭️ **Alert Checking**: Skipped" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Dashboard generation status
          if [ "${{ needs.generate-dashboard.result }}" = "success" ]; then
            echo "✅ **Dashboard Generation**: Completed" >> $GITHUB_STEP_SUMMARY
            echo "   📊 [View Performance Dashboard](https://github.com/${{ github.repository }}/actions/artifacts)" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ needs.generate-dashboard.result }}" = "failure" ]; then
            echo "❌ **Dashboard Generation**: Failed" >> $GITHUB_STEP_SUMMARY
          else
            echo "⏭️ **Dashboard Generation**: Skipped" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Overall status and recommendations
          CRITICAL_ALERTS="${{ needs.check-alerts.outputs.critical-alerts }}"
          
          if [ "$CRITICAL_ALERTS" -gt 0 ]; then
            echo "## 🚨 Action Required" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Critical performance alerts detected. Please:" >> $GITHUB_STEP_SUMMARY
            echo "1. Review the created GitHub issue for details" >> $GITHUB_STEP_SUMMARY
            echo "2. Check the performance dashboard for trends" >> $GITHUB_STEP_SUMMARY
            echo "3. Investigate and optimize slow tests" >> $GITHUB_STEP_SUMMARY
            echo "4. Consider infrastructure improvements if needed" >> $GITHUB_STEP_SUMMARY
          else
            echo "## ✅ Performance Status: Good" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "All performance metrics are within acceptable thresholds." >> $GITHUB_STEP_SUMMARY
            echo "Continue monitoring for any degradation trends." >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### 📊 Resources" >> $GITHUB_STEP_SUMMARY
          echo "- [Performance Dashboard Artifact](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> $GITHUB_STEP_SUMMARY
          echo "- [Metrics Collection Script](https://github.com/${{ github.repository }}/blob/main/scripts/test-performance-dashboard.js)" >> $GITHUB_STEP_SUMMARY
          echo "- [Performance Monitoring Workflow](https://github.com/${{ github.repository }}/blob/main/.github/workflows/test-performance-monitoring.yml)" >> $GITHUB_STEP_SUMMARY