name: 🔍 Flaky Test Detection

on:
  schedule:
    # Run flaky test detection weekly on Sundays at 2 AM UTC
    - cron: '0 2 * * 0'
  workflow_dispatch:
    inputs:
      suite:
        description: 'Test suite to analyze (all, unit, integration, e2e)'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - unit
          - integration
          - e2e
      runs:
        description: 'Number of test runs per suite'
        required: false
        default: '5'
        type: string

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  NODE_ENV: test
  JWT_SECRET: test-secret-key-for-ci-environment-must-be-longer-than-32-characters
  DATABASE_URL: file:./prisma/flaky-test.db

jobs:
  detect-flaky-tests:
    name: 🔍 Detect Flaky Tests
    runs-on: ubuntu-latest
    timeout-minutes: 90

    steps:
      - name: 📥 Checkout code
        uses: actions/checkout@v4

      - name: 📦 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'

      - name: 💾 Cache dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.npm
            node_modules
          key: deps-${{ runner.os }}-${{ hashFiles('package-lock.json') }}
          restore-keys: |
            deps-${{ runner.os }}-

      - name: 📥 Install dependencies
        run: npm ci

      - name: 📦 Install Playwright browsers
        run: npx playwright install --with-deps chromium

      - name: 🗄️ Setup test database
        run: |
          mkdir -p prisma
          rm -f prisma/flaky-test.db*
          npx prisma migrate dev --name init || npx prisma migrate deploy
          npx prisma generate

      - name: 💾 Restore flaky test data
        uses: actions/cache@v4
        with:
          path: test-flakiness/
          key: flaky-tests-${{ github.sha }}
          restore-keys: |
            flaky-tests-

      - name: 🔍 Run Comprehensive Flaky Test Detection
        run: |
          echo "🔍 Running comprehensive flaky test detection..."
          
          # Set up environment for the tracker
          export FLAKY_OUTPUT_DIR="./test-flakiness"
          export FLAKY_THRESHOLD="0.1"
          export FLAKY_RETRY_COUNT="${{ github.event.inputs.runs || '3' }}"
          export FLAKY_HISTORY_LENGTH="50"
          
          # Create output directory
          mkdir -p test-flakiness
          
          # Determine which suite to run
          SUITE="${{ github.event.inputs.suite || 'all' }}"
          echo "Running flaky test detection for suite: $SUITE"
          
          # Run the comprehensive flaky test tracker
          node scripts/flaky-test-tracker.js track "$SUITE"

      - name: 📊 Generate Additional Analysis
        run: |
          echo "📊 Generating additional analysis and reports..."
          
          # Generate retry configuration
          node scripts/flaky-test-tracker.js config
          
          # Create comprehensive analysis
          node scripts/flaky-test-tracker.js analyze

      - name: 💾 Save flaky test data
        uses: actions/cache@v4
        with:
          path: test-flakiness/
          key: flaky-tests-${{ github.sha }}

      - name: 📤 Upload flaky test artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: flaky-test-analysis
          path: |
            test-flakiness/
          retention-days: 30
          
      - name: 📤 Upload retry configuration
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: flaky-test-retry-config
          path: test-flakiness/retry-config.json
          retention-days: 90

      - name: 📊 Flaky Test Summary
        if: always()
        run: |
          echo "# 🔍 Flaky Test Detection Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f "test-flakiness/flaky-tests-report.json" ]; then
            echo "## 📊 Test Statistics" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            # Extract statistics from the new tracker format
            TOTAL_TESTS=$(jq '.summary.total' test-flakiness/flaky-tests-report.json 2>/dev/null || echo "0")
            FLAKY_TESTS=$(jq '.summary.flakyCount' test-flakiness/flaky-tests-report.json 2>/dev/null || echo "0")
            FLAKY_PERCENTAGE=$(jq '.summary.flakyPercentage' test-flakiness/flaky-tests-report.json 2>/dev/null || echo "0")
            HIGH_RISK=$(jq '[.flakyTests[] | select(.severity == "high")] | length' test-flakiness/flaky-tests-report.json 2>/dev/null || echo "0")
            MEDIUM_RISK=$(jq '[.flakyTests[] | select(.severity == "medium")] | length' test-flakiness/flaky-tests-report.json 2>/dev/null || echo "0")
            LOW_RISK=$(jq '[.flakyTests[] | select(.severity == "low")] | length' test-flakiness/flaky-tests-report.json 2>/dev/null || echo "0")
            CONSISTENTLY_FAILING=$(jq '.consistentlyFailing | length' test-flakiness/flaky-tests-report.json 2>/dev/null || echo "0")
            
            echo "| Metric | Count |" >> $GITHUB_STEP_SUMMARY
            echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
            echo "| Total Tests Tracked | $TOTAL_TESTS |" >> $GITHUB_STEP_SUMMARY
            echo "| Flaky Tests | $FLAKY_TESTS ($FLAKY_PERCENTAGE%) |" >> $GITHUB_STEP_SUMMARY
            echo "| High Severity | $HIGH_RISK |" >> $GITHUB_STEP_SUMMARY
            echo "| Medium Severity | $MEDIUM_RISK |" >> $GITHUB_STEP_SUMMARY
            echo "| Low Severity | $LOW_RISK |" >> $GITHUB_STEP_SUMMARY
            echo "| Consistently Failing | $CONSISTENTLY_FAILING |" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            if [ "$FLAKY_TESTS" -gt "0" ]; then
              echo "## 🚨 Top Flaky Tests" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "```" >> $GITHUB_STEP_SUMMARY
              jq -r '.flakyTests | sort_by(.flakyScore) | reverse | .[0:5] | .[] | "• \(.testName) (\(.suite)) - Score: \(.flakyScore), Success: \(100 - (.failureRate | tonumber))%, Severity: \(.severity)"' test-flakiness/flaky-tests-report.json 2>/dev/null >> $GITHUB_STEP_SUMMARY || echo "Unable to parse flaky test details" >> $GITHUB_STEP_SUMMARY
              echo "```" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
            else
              echo "🎉 **No flaky tests detected!** Your test suite is stable." >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
            fi
            
            echo "## 📋 Recommendations" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            jq -r '.recommendations[] | "- \(.)"' test-flakiness/flaky-tests-report.json 2>/dev/null >> $GITHUB_STEP_SUMMARY || echo "- Review full report in artifacts for detailed recommendations"
            echo "" >> $GITHUB_STEP_SUMMARY
            
            echo "## 📊 Resources" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "- [Flaky Test Dashboard](${GITHUB_SERVER_URL}/${GITHUB_REPOSITORY}/actions/runs/${GITHUB_RUN_ID})" >> $GITHUB_STEP_SUMMARY
            echo "- [Retry Configuration](${GITHUB_SERVER_URL}/${GITHUB_REPOSITORY}/actions/runs/${GITHUB_RUN_ID})" >> $GITHUB_STEP_SUMMARY
            echo "- Full HTML dashboard available in artifacts" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ Flaky test data not generated" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "This may be the first run or there was an issue with data collection." >> $GITHUB_STEP_SUMMARY
          fi

      - name: 🚨 Create issue for high-risk flaky tests
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            try {
              // Check if we have flaky test data
              if (!fs.existsSync('test-flakiness/flaky-tests-report.json')) {
                console.log('No flaky test report found, skipping issue creation');
                return;
              }
              
              const report = JSON.parse(fs.readFileSync('test-flakiness/flaky-tests-report.json', 'utf8'));
              const highRiskTests = report.flakyTests ? report.flakyTests.filter(test => test.severity === 'high') : [];
              
              if (highRiskTests.length === 0) {
                console.log('No high-risk flaky tests found');
                return;
              }
              
              // Check if similar issue already exists
              const existingIssues = await github.rest.issues.listForRepo({
                owner: context.repo.owner,
                repo: context.repo.repo,
                labels: 'flaky-tests,high-risk',
                state: 'open'
              });
              
              const recentIssue = existingIssues.data.find(issue => 
                issue.title.includes('High-Risk Flaky Tests') &&
                Date.now() - new Date(issue.created_at).getTime() < 7 * 24 * 60 * 60 * 1000 // Less than 7 days old
              );
              
              if (recentIssue) {
                console.log(`Recent flaky test issue already exists: #${recentIssue.number}`);
                
                // Add comment to existing issue
                await github.rest.issues.createComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  issue_number: recentIssue.number,
                  body: `## 🔄 Updated Flaky Test Detection - ${new Date().toLocaleDateString()}
                  
                  **High-Risk Tests Found**: ${highRiskTests.length}
                  **Total Flaky Tests**: ${report.summary.flakyCount}
                  **Flaky Percentage**: ${report.summary.flakyPercentage}%
                  
                  ### 🚨 Current High-Risk Tests:
                  ${highRiskTests.slice(0, 5).map(test => 
                    `- **${test.testName}** (${test.suite})
                      - Flaky Score: ${test.flakyScore.toFixed(3)}
                      - Success Rate: ${(100 - parseFloat(test.failureRate)).toFixed(1)}%
                      - Total Runs: ${test.totalRuns}
                      - Severity: ${test.severity}`
                  ).join('\n')}
                  
                  Please review the full analysis in the workflow artifacts.`
                });
                
                return;
              }
              
              // Create new issue
              const issueBody = `# 🚨 High-Risk Flaky Tests Detected
              
              **Detection Date**: ${new Date().toLocaleDateString()}
              **Workflow Run**: [View Details](${context.payload.repository.html_url}/actions/runs/${context.runId})
              
              ## 📊 Summary
              
              - **Total Tests Tracked**: ${report.summary.total}
              - **Flaky Tests**: ${report.summary.flakyCount} (${report.summary.flakyPercentage}%)
              - **High-Risk Flaky Tests**: ${highRiskTests.length}
              - **Consistently Failing**: ${report.consistentlyFailing.length}
              
              ## 🚨 High-Risk Tests
              
              ${highRiskTests.map((test, index) => `
              ### ${index + 1}. ${test.testName}
              
              - **Suite**: ${test.suite.toUpperCase()}
              - **Flaky Score**: ${test.flakyScore.toFixed(3)}
              - **Success Rate**: ${(100 - parseFloat(test.failureRate)).toFixed(1)}%
              - **Total Runs**: ${test.totalRuns}
              - **Severity**: ${test.severity}
              - **Recent Pattern**: ${test.recentRuns ? test.recentRuns.length : 'N/A'} recent runs tracked
              `).join('\n')}
              
              ## 🔍 Recommended Actions
              
              ### Immediate (High Priority)
              - [ ] **Quarantine high-risk tests** - Consider skipping these tests temporarily
              - [ ] **Review test implementations** for race conditions and timing issues
              - [ ] **Check test isolation** - Ensure tests don't affect each other
              - [ ] **Analyze failure patterns** - Look for common causes across flaky tests
              
              ### Investigation Steps
              - [ ] Run tests locally multiple times to reproduce flakiness
              - [ ] Check for external dependencies (APIs, databases, file system)
              - [ ] Review async/await usage and promise handling
              - [ ] Verify proper cleanup and teardown procedures
              - [ ] Check for shared state or global variables
              
              ### Fix Strategies
              - [ ] Add proper wait conditions and timeouts
              - [ ] Implement retry logic for external dependencies
              - [ ] Use test fixtures and mocks to reduce external factors
              - [ ] Improve test data setup and cleanup
              - [ ] Add explicit synchronization points
              
              ## 📈 Monitoring
              
              This issue will be updated weekly with new flaky test detection results.
              For detailed analysis, check the workflow artifacts.
              
              ---
              
              *This issue was automatically created by the Flaky Test Detection workflow.*`;
              
              await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: `🚨 High-Risk Flaky Tests Detected - ${new Date().toLocaleDateString()}`,
                body: issueBody,
                labels: ['flaky-tests', 'high-risk', 'automated', 'needs-investigation', 'testing']
              });
              
              console.log(`Created issue for ${highRiskTests.length} high-risk flaky tests`);
              
            } catch (error) {
              console.error('Error creating flaky test issue:', error);
            }

      - name: 🧹 Cleanup
        if: always()
        run: |
          rm -f prisma/flaky-test.db*
          echo "🧹 Cleanup completed"
