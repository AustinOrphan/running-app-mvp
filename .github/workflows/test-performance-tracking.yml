name: üìä Test Performance Tracking

on:
  workflow_run:
    workflows: ['üß™ CI Pipeline']
    types: [completed]
  schedule:
    # Run performance tracking daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: false # Don't cancel performance tracking

env:
  NODE_ENV: test
  JWT_SECRET: test-secret-key-for-ci-environment-must-be-longer-than-32-characters
  DATABASE_URL: file:./prisma/performance-test.db

jobs:
  track-performance:
    name: üìà Track Test Performance
    runs-on: ubuntu-latest
    if: always() # Run even if triggering workflow failed

    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4

      - name: üì¶ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'

      - name: üíæ Cache dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.npm
            node_modules
          key: deps-${{ runner.os }}-${{ hashFiles('package-lock.json') }}
          restore-keys: |
            deps-${{ runner.os }}-

      - name: üì• Install dependencies
        run: npm ci

      - name: üóÑÔ∏è Setup test database
        run: |
          mkdir -p prisma
          rm -f prisma/performance-test.db*
          npx prisma migrate dev --name init || npx prisma migrate deploy
          npx prisma generate

      - name: üìä Track unit test performance
        continue-on-error: true
        run: |
          echo "üß™ Tracking unit test performance..."
          tsx scripts/test-performance-tracker.ts track unit

      - name: üìä Track integration test performance
        continue-on-error: true
        run: |
          echo "üîó Tracking integration test performance..."
          tsx scripts/test-performance-tracker.ts track integration

      - name: üìä Track E2E test performance
        continue-on-error: true
        run: |
          echo "üåê Setting up browser for E2E tests..."
          npx playwright install --with-deps chromium
          echo "üé≠ Tracking E2E test performance..."
          tsx scripts/test-performance-tracker.ts track e2e

      - name: üìà Generate performance report
        run: |
          echo "üìã Generating performance report..."
          tsx scripts/test-performance-tracker.ts report

      - name: üìä Generate performance dashboard
        run: |
          echo "üé® Generating performance dashboard..."
          tsx scripts/test-performance-dashboard.ts generate

      - name: üíæ Cache performance data
        uses: actions/cache@v4
        with:
          path: test-performance-data/
          key: performance-data-${{ github.sha }}-${{ github.run_number }}
          restore-keys: |
            performance-data-${{ github.sha }}-
            performance-data-

      - name: üì§ Upload performance artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-performance-report
          path: |
            test-performance-data/
            test-results/
          retention-days: 30

      - name: üìä Performance Summary
        if: always()
        run: |
          echo "# üìä Test Performance Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f "test-performance-data/test-results.json" ]; then
            echo "## üìà Latest Performance Metrics" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            # Extract summary data (simplified - would normally parse JSON)
            echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
            echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
            echo "| Tracking Run | $(date) |" >> $GITHUB_STEP_SUMMARY
            echo "| Performance Data | Available in artifacts |" >> $GITHUB_STEP_SUMMARY
            echo "| Dashboard | Generated successfully |" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            echo "## üìã Performance Report" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "```" >> $GITHUB_STEP_SUMMARY
            tsx scripts/test-performance-tracker.ts report 2>/dev/null | head -30 >> $GITHUB_STEP_SUMMARY || echo "Report generation completed"
            echo "```" >> $GITHUB_STEP_SUMMARY
            
            echo "## üîó Next Steps" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "- Download performance artifacts to view detailed dashboard" >> $GITHUB_STEP_SUMMARY
            echo "- Check for performance regressions in the report above" >> $GITHUB_STEP_SUMMARY
            echo "- Review slowest tests for optimization opportunities" >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ùå Performance tracking data not generated" >> $GITHUB_STEP_SUMMARY
          fi

      - name: üßπ Cleanup
        if: always()
        run: |
          rm -f prisma/performance-test.db*
          echo "üßπ Cleanup completed"

  # Job to analyze performance trends and create issues for regressions
  analyze-trends:
    name: üìâ Analyze Performance Trends
    needs: track-performance
    runs-on: ubuntu-latest
    if: always()

    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4

      - name: üì¶ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'

      - name: üíæ Restore performance data
        uses: actions/cache@v4
        with:
          path: test-performance-data/
          key: performance-data-${{ github.sha }}-${{ github.run_number }}
          restore-keys: |
            performance-data-${{ github.sha }}-
            performance-data-

      - name: üì• Install dependencies
        run: npm ci

      - name: üìà Analyze performance trends
        id: trends
        run: |
          echo "üìä Analyzing performance trends..."

          if [ -f "test-performance-data/test-results.json" ]; then
            # Generate trends analysis
            tsx scripts/test-performance-tracker.ts trends > trends.json
            
            # Check for significant regressions (simplified check)
            if grep -q '"trend": "degrading"' trends.json; then
              echo "regression=true" >> $GITHUB_OUTPUT
              echo "‚ö†Ô∏è Performance regression detected!"
            else
              echo "regression=false" >> $GITHUB_OUTPUT
              echo "‚úÖ No significant performance regressions detected"
            fi
          else
            echo "regression=false" >> $GITHUB_OUTPUT
            echo "‚ùå No performance data available for trend analysis"
          fi

      - name: üö® Create performance regression issue
        if: steps.trends.outputs.regression == 'true' && github.event_name != 'workflow_dispatch'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            // Read trends data
            let trendsData = [];
            try {
              if (fs.existsSync('trends.json')) {
                trendsData = JSON.parse(fs.readFileSync('trends.json', 'utf8'));
              }
            } catch (error) {
              console.log('Could not read trends data:', error);
            }

            // Find degrading trends
            const degradingTrends = trendsData.filter(trend => trend.trend === 'degrading');

            if (degradingTrends.length > 0) {
              const issueBody = `# üìâ Performance Regression Detected

            **Detected:** ${new Date().toISOString()}
            **Commit:** ${context.sha}
            **Workflow:** ${context.workflow}

            ## üö® Degrading Performance Trends

            ${degradingTrends.map(trend => 
              `- **${trend.suite}** ${trend.metric}: ${trend.changePercent.toFixed(1)}% degradation (${trend.samples} samples)`
            ).join('\n')}

            ## üìä Analysis

            This issue was automatically created when performance tracking detected significant regressions in test execution times or pass rates.

            ## üîç Investigation Steps

            1. Review the performance dashboard in the workflow artifacts
            2. Identify which specific tests are causing the regression
            3. Check recent code changes that might impact performance
            4. Consider optimizing slow tests or infrastructure changes

            ## üìà Tracking

            - [ ] Investigate root cause
            - [ ] Implement performance improvements
            - [ ] Verify improvements with subsequent test runs
            - [ ] Update performance baselines if needed

            ---

            *This issue was automatically created by the Test Performance Tracking workflow.*`;

              await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: `üö® Performance Regression Detected - ${new Date().toLocaleDateString()}`,
                body: issueBody,
                labels: ['performance', 'regression', 'automated']
              });
              
              console.log('Created performance regression issue');
            }

      - name: üí¨ Performance trend comment
        if: github.event_name == 'workflow_run' && github.event.workflow_run.event == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            // This would add a comment to the PR with performance insights
            // For now, just log the trends
            console.log('Performance trends analysis completed');

            if (fs.existsSync('trends.json')) {
              const trends = JSON.parse(fs.readFileSync('trends.json', 'utf8'));
              console.log('Performance trends:', JSON.stringify(trends, null, 2));
            }
